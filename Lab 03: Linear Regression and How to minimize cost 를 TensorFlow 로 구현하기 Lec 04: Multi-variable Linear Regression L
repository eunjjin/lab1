#Lab3 – How to minimize cost LAB
#비용최소화 Cost minimize를 tensorflow로 구현

#<<<<<<<<<<<cost를 python으로 구현해보기>>>>>>>>>>>>>>>

import numpy as np  #numpy를 np으로 지정

X = np.array([1, 2, 3])             # 입력값 X= [1,2,3] 
Y = np.array([1, 2, 3])             # 출력값 Y= [1,2,3]  

def cost_func(W, X, Y):         # X,Y에 대한 W값이 주어졌을 때 cost를 계산하는 함수 
 c = 0
    for i in range(len(X)):
        c += (W * X[i] - Y[i]) ** 2       
 # W * X[i] = 우리의 예측값 
    Y[i] = 실체값
따라서   W * X[i] - Y[i] = 오차 

    return c / len(X)            
#그 오차를 제곱한 값을 c에 누적 덧셈하여 데이터의 개수로 나누어줌 = 평균을 낸 것 
             
for feed_W in np.linspace(-3, 5, num=15):    
# np.linspace라는 함수에서 시작(-3)과 끝(5)을 지정하고 이 사이를 15개의 구간으로 나누어서 feed_W의 값은-3에서 5까지의 15개의 구간 값을 가지게 된다. 
    curr_cost = cost_func(feed_W, X, Y)    
  #feed_W에 따라서 cost가 얼마가 나오는지 출력   
    print("{:6.3f} | {:10.5f}".format(feed_W, curr_cost))
#출력값
#  w        cost
-3.000 |   74.66667
-2.429 |   54.85714                        # cost는 w값에 따라 바뀌고 있다. 
-1.857 |   38.09524 
-1.286 |   24.38095
-0.714 |   13.71429
-0.143 |    6.09524
 0.429 |    1.52381
 1.000 |    0.00000
 1.571 |    1.52381
 2.143 |    6.09524
 2.714 |   13.71429
 3.286 |   24.38095
 3.857 |   38.09524
 4.429 |   54.85714
 5.000 |   74.66667
 
 
#<<<<<<<<<<<<위 내용을 tensorflow로 동일하게 구현해보기>>>>>>>>>>
내용은 거의 동일, but 구현한 함수가 좀 차이가 있음

X = np.array([1, 2, 3])   # 입력값 X= [1,2,3] 
Y = np.array([1, 2, 3])   # 출력값 Y= [1,2,3] 

def cost_func(W, X, Y):    #cost 함수식 구현하기(파이썬과 다른 부분)
  hypothesis = X * W       #가설은 X*Y
  return tf.reduce_mean(tf.square(hypothesis – Y))    
#우리의 cost_func =
hypothesis에서 y를 빼고 그것을 제곱(square)하여 평균(reduce.mean)을 낸다.

W_values = np.linspace(-3, 5, num=15)  
cost_values = []
#np의 linspace를 활용하여 -3에서 5까지의 구간을 15개로 쪼개고 그 값을 list로 받는다

for feed_W in W_values :        
#받은 값을 하나하나 뽑아 
    curr_cost = cost_func(feed_W, X, Y)  
    cost_values.append(curr_cost)
    print("{:6.3f} | {:10.5f}".format(feed_W, curr_cost))
 # W값으로 사용하여 코스트가 W에 따라서 어떻게 변하는지를 기록했다가 출력을 해본다.   

#출력값
#  w       cost
-3.000 |   74.66667
-2.429 |   54.85714
-1.857 |   38.09524
-1.286 |   24.38095
-0.714 |   13.71429
-0.143 |    6.09524
 0.429 |    1.52381
 1.000 |    0.00000
 1.571 |    1.52381
 2.143 |    6.09524
 2.714 |   13.71429
 3.286 |   24.38095
 3.857 |   38.09524
 4.429 |   54.85714
 5.000 |   74.66667
 
#python으로 했을 때와 똑같이 나옴
  
#<<<<<<Gradient decsent를 tensorflow코드로 그대로 옮길 수 있다.>>>>>>>>>>

 alpha = 0.01
 gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, X) - Y, X))
 #(WX –Y) 한 것에 X를 곱하고 이 값의 평균을 구한다. 이렇게가 gradient=기울기
 descent = W – tf.multiply(alpha,gradient) 
#이렇게 구한 gradient에 alpha값을 곱하고 W에서 빼준다. 
이렇게 만들어진 게 새로운 W값이다= descent
 W.assign(descent)  
#새로운 W값을 W에 할당함으로써 업데이트한다.
 

 #Gradient decsent를 실제로 적용하여 tensorflow코드로 구현하기
 

 
tf.set_random_seed(0)      
#random_seed를 초기화시킨다. 다음에 코드를 다시수행했을 때도 동일하게 똑같이 재현될 수 있도록 하기위해 random_seed 특정한 값으로 초기화를 시킨다.

#데이터를 준비한다 x_data, y_data
x_data = [1., 2., 3., 4.]   # x =[1, 2, 3, 4]
y_data = [1., 3., 5., 7.]   # y =[1, 3, 5, 7]

W = tf.Variable(tf.random_normal([1], -100., 100.))  
#tf.Variablefh W를 정의한다, 이때 정규분포를 따르는 random number를 1개짜리(= 모양)로 변수를 만들어서 w에 할당해서 정의한다.

for step in range(300):  
#  Gradient decsent 300회 수행한다.=300step진행
    hypothesis = W * X    
    cost = tf.reduce_mean(tf.square(hypothesis - Y))
#cost는 (hypothesis – Y)=즉 차이의 평균으로 정의

    alpha = 0.01
 gradient = tf.reduce_mean(tf.multiply(tf.multiply(W, X) - Y, X))
 #(WX –Y) 한 것에 X를 곱하고 이 값의 평균을 구한다. 이렇게가 gradient=기울기
 descent = W – tf.multiply(alpha,gradient) 
#이렇게 구한 gradient에 alpha값을 곱하고 W에서 빼준다. 
이렇게 만들어진 게 새로운 W값이다= descent
 W.assign(descent)  
#새로운 W값을 W에 할당함으로써 업데이트한다.
       .     
     
    if step % 10 == 0:       
#300회을 수행하면서10번에 한번씩 cost값과 W값을 출력을 해본다
        print('{:5} | {:10.4f} | {:10.6f}'.format(
            step, cost.numpy(), W.numpy()[0]))

            
#W값과 cost값을 출력 
# step      cost         W
    0 | 11716.3086 |  48.767971
   10 |  4504.9126 |  30.619968
   20 |  1732.1364 |  19.366755
   30 |   666.0052 |  12.388859
   40 |   256.0785 |   8.062004
   50 |    98.4620 |   5.379007
   60 |    37.8586 |   3.715335
   70 |    14.5566 |   2.683725
   80 |     5.5970 |   2.044044
   90 |     2.1520 |   1.647391
  100 |     0.8275 |   1.401434
  110 |     0.3182 |   1.248922
  120 |     0.1223 |   1.154351
  130 |     0.0470 |   1.095710
  140 |     0.0181 |   1.059348
  150 |     0.0070 |   1.036801
  160 |     0.0027 |   1.022819
  170 |     0.0010 |   1.014150
  180 |     0.0004 |   1.008774
  190 |     0.0002 |   1.005441
  200 |     0.0001 |   1.003374
  210 |     0.0000 |   1.002092
  220 |     0.0000 |   1.001297
  230 |     0.0000 |   1.000804
  240 |     0.0000 |   1.000499
  250 |     0.0000 |   1.000309
  260 |     0.0000 |   1.000192
  270 |     0.0000 |   1.000119
  280 |     0.0000 |   1.000074
  290 |     0.0000 |   1.000046

#step을 진행하며 cost와 W가 어떻게 바뀌어가고 있는지 살펴본다.
cost는 굉장히 큰 값을 갖고있다가 점점 0에 수렴하는 것을 알 수 있다. 
w역시 처음엔 임의의 값이었다가 특정 값으로 수렴해가는 것을 볼 수 있다.
하지만 w에 랜덤값말고 특정값을 주어도 실제로 결과는 동일하게 나타난다. 
결과적으로 w값이 무엇이든 상관없이 cost는 0으로 수렴하고 w는 특절한 값으로 수렴하는 것을 볼 수 있다. 

